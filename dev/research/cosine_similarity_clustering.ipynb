{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T17:39:09.373658Z",
     "start_time": "2024-04-13T17:39:07.898007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datetime import datetime as dt, timedelta as td\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from app.models import Session, Article, Agency, Headline, Topic\n",
    "import pytz\n",
    "from sqlalchemy import or_\n",
    "from app.utils.config import Config\n",
    "from app.utils.constants import Country\n",
    "import pandas as pd\n",
    "\n",
    "with Session() as s:\n",
    "    headlines = s.query(Headline.processed, Article.url, Agency.name, Agency._bias).join(Headline.article).join(Article.agency)\\\n",
    "        .filter(Headline.first_accessed > (dt.now(tz=pytz.UTC) - td(hours=24)),\n",
    "                or_(Agency._country == Country.us.value, Agency.name.in_(Config.exempted_foreign_media))).all()\n",
    "    df = pd.DataFrame(headlines, columns=['headline', 'url', 'agency', 'bias'])\n",
    "df.head()"
   ],
   "id": "865235c9f4b13c1c",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T17:39:09.380901Z",
     "start_time": "2024-04-13T17:39:09.374661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in tokens if word not in stop_words])"
   ],
   "id": "b43252d8234e7c20",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T17:39:23.908371Z",
     "start_time": "2024-04-13T17:39:09.381899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "X = df.copy()\n",
    "X['processed'] = X['headline'].apply(preprocess)\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(X['processed'])\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "sim_df = pd.DataFrame(cosine_sim, columns=X.index, index=X.index)\n",
    "np.fill_diagonal(sim_df.values, 0)\n",
    "top_indices = pd.DataFrame(sim_df.unstack(), columns=['similarity']).sort_values(by='similarity', ascending=False)\n",
    "top_pairs = top_indices.sort_values(by='similarity', ascending=False)[top_indices['similarity'] > 0]\n",
    "top_pairs = top_pairs.reset_index().rename(columns={'level_0': 'index1', 'level_1': 'index2'})\n",
    "def copy_col(fromdf, todf, col):\n",
    "    todf[col + '1'] = todf['index1'].apply(lambda x: fromdf.loc[x, col])\n",
    "    todf[col + '2'] = todf['index2'].apply(lambda x: fromdf.loc[x, col])\n",
    "    \n",
    "copy_col(X, top_pairs, 'headline')\n",
    "copy_col(X, top_pairs, 'agency')"
   ],
   "id": "f932574be17d1c44",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T17:39:23.915249Z",
     "start_time": "2024-04-13T17:39:23.909369Z"
    }
   },
   "cell_type": "code",
   "source": "top_pairs",
   "id": "3ffa7cbc1312e1f9",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T17:39:23.963945Z",
     "start_time": "2024-04-13T17:39:23.916246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# drop where agency1 and agency2 are equal\n",
    "top_pairs = top_pairs[top_pairs['agency1'] != top_pairs['agency2']]\n",
    "top_pairs.head()"
   ],
   "id": "6b7a8c640b482e00",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T17:39:23.967323Z",
     "start_time": "2024-04-13T17:39:23.963945Z"
    }
   },
   "cell_type": "code",
   "source": "len(top_pairs)",
   "id": "a9fe8658c914e2a7",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T17:39:23.974579Z",
     "start_time": "2024-04-13T17:39:23.967323Z"
    }
   },
   "cell_type": "code",
   "source": "top_pairs[top_pairs['similarity']> 0.5].sort_values('similarity', ascending=True).head(10)",
   "id": "bc34b66f9b90a129",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T17:39:23.978835Z",
     "start_time": "2024-04-13T17:39:23.974579Z"
    }
   },
   "cell_type": "code",
   "source": "len(top_pairs[top_pairs['similarity'] > 0.5])",
   "id": "d90afbd6b0ccd5d5",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T17:39:23.981632Z",
     "start_time": "2024-04-13T17:39:23.978835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_all_similar(df, id):\n",
    "    allsim = df[(df['index1'] == id) | (df['index2'] == id)]\n",
    "    return allsim[allsim['similarity'] > 0.2].copy()"
   ],
   "id": "71a00520a9da7b1a",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T17:39:23.990750Z",
     "start_time": "2024-04-13T17:39:23.982637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hezbollah = get_all_similar(top_pairs, 1880)\n",
    "hezbollah"
   ],
   "id": "3acc2b1534048ab3",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T17:39:24.292757Z",
     "start_time": "2024-04-13T17:39:23.990750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TF-IDF Vectorization\n",
    "X = df.copy()\n",
    "X['text'] = X['headline'].apply(preprocess)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(X['text'])\n",
    "\n",
    "# Cosine Similarity\n",
    "cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Thresholding\n",
    "threshold = 0.5\n",
    "cosine_sim[cosine_sim < threshold] = 0\n"
   ],
   "id": "b21bf4c47f156d1c",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T17:40:13.403203Z",
     "start_time": "2024-04-13T17:39:24.293762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import deque\n",
    "# Function to form clusters\n",
    "def form_clusters(cosine_sim, min_samples=10):\n",
    "    clusters = []\n",
    "    last_n = 0\n",
    "    while np.any(cosine_sim):\n",
    "        # Start with the first available row\n",
    "        first_index = np.where(np.any(cosine_sim, axis=1))[0][0]\n",
    "        # Get all indices where similarity >= threshold\n",
    "        involved_indices = np.where(cosine_sim[first_index] >= threshold)[0]\n",
    "        # Add all connected components\n",
    "        queue = deque(involved_indices)\n",
    "        cluster = set(queue)\n",
    "        while queue:\n",
    "            current = queue.popleft()\n",
    "            connected_indices = np.where(cosine_sim[current] >= threshold)[0]\n",
    "            new_indices = set(connected_indices) - cluster\n",
    "            queue.extend(new_indices)\n",
    "            cluster.update(new_indices)\n",
    "        # Mark these rows and columns as processed\n",
    "        for idx in cluster:\n",
    "            cosine_sim[idx, :] = 0\n",
    "            cosine_sim[:, idx] = 0\n",
    "        # Store the cluster\n",
    "        if len(cluster) >= 10:\n",
    "            clusters.append(cluster)\n",
    "        if len(clusters) > last_n:\n",
    "            last_n = len(clusters)\n",
    "            print(f\"Clusters formed: {len(clusters)}\")\n",
    "        \n",
    "    return clusters\n",
    "\n",
    "# Forming clusters\n",
    "clusters = form_clusters(cosine_sim.copy())\n"
   ],
   "id": "9fb530d05230b04d",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T17:40:13.420528Z",
     "start_time": "2024-04-13T17:40:13.404206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display clusters\n",
    "for idx, cluster in enumerate(clusters):\n",
    "    print(f\"Cluster {idx + 1}:\")\n",
    "    for doc_idx in cluster:\n",
    "        row = X.iloc[doc_idx]\n",
    "        print(f\" {row['agency']} - {row['headline']}\")\n",
    "    print()"
   ],
   "id": "6ab7c97f3ffead3f",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T17:40:13.427850Z",
     "start_time": "2024-04-13T17:40:13.421530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cluster_labels = [-1] * len(X)\n",
    "for cluster_id, cluster_indices in enumerate(clusters):\n",
    "    for idx in cluster_indices:\n",
    "        cluster_labels[idx] = cluster_id\n",
    "X['cluster'] = cluster_labels\n",
    "X"
   ],
   "id": "6c8c0a509721c9cd",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T17:40:13.434679Z",
     "start_time": "2024-04-13T17:40:13.427850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "clusters_to_drop = X.groupby('cluster').filter(lambda x: x['agency'].nunique() == 1)\n",
    "clusters_to_drop"
   ],
   "id": "de55906b691f3a38",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T17:40:13.441354Z",
     "start_time": "2024-04-13T17:40:13.434679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "drop_cluster_ids = clusters_to_drop['cluster'].unique()\n",
    "X_filtered = X[~X['cluster'].isin(drop_cluster_ids)]\n",
    "X_filtered"
   ],
   "id": "f8ce0e0e0f9c05fe",
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T17:40:13.447744Z",
     "start_time": "2024-04-13T17:40:13.441354Z"
    }
   },
   "cell_type": "code",
   "source": "X.sort_values('cluster')",
   "id": "4a2bbbbb5ba87f70",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T17:40:13.454274Z",
     "start_time": "2024-04-13T17:40:13.448748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = X[X['cluster'] != -1]\n",
    "X.sort_values('cluster')"
   ],
   "id": "61569002a451a5c3",
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T17:40:13.456598Z",
     "start_time": "2024-04-13T17:40:13.454274Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ed58b338837c572a",
   "execution_count": 19,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
